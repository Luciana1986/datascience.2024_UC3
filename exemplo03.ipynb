{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar bibliotecas,\n",
    "# tensorflow é uma biblioteca de código aberto para aprendizado de máquina \n",
    "# e aprendizado profundopara treinamento e inferência de redes neurais artificiais.\n",
    "# https://www.tensorflow.org/\n",
    "# %pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "\n",
    "\n",
    "# constante do endereco do arquivo\n",
    "ENDERECO_DADOS = 'https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Obtendo dados...')\n",
    "    df = pd.read_csv(ENDERECO_DADOS,sep=',',encoding='utf-8')[['review_title','overall_rating']]  \n",
    "# delimitando colunas para criação do modelo\\n\",\n",
    "    df = df.dropna(subset=['review_title','overall_rating'])\n",
    "    \n",
    "# transformando colunas em arrays\\n\",\n",
    "    texts = np.array(df['review_title'])\n",
    "    ratings = np.array(df['overall_rating'])  \n",
    "    \n",
    "    print(df.head())\n",
    "    print(len(df))\n",
    "    print(df['overall_rating'].unique())\n",
    "    print(df['overall_rating'].value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Errro ao obter dados: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetorização de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Vetorizando texto...')\n",
    " # Passo 1: Tokenizar os textos\n",
    "    tokenizer = Tokenizer()\n",
    "# fit_on_texts: cria o vocabulário\n",
    "#vocabulário:dicionário que mapeia palavras para números\n",
    "# cada palavra é mapeada para um número inteiro e \n",
    "# esse número inteiro é o índice da palavra no vocabulário\n",
    "# toda vez que uma palavra aparece em um texto,\n",
    "# ela é substituída pelo seu índice no vocabulário\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "# Converter os textos em sequências de números\n",
    "# texts_to_sequences: converte os textos em sequências de números\n",
    "# É a vetorização dos textos. Cada texto é convertido em um,\n",
    "# vetor de números inteiros, onde cada número inteiro representa\n",
    " # uma palavra do texto, baseada no índice da palavra no vocabulário\n",
    "    vetores = tokenizer.texts_to_sequences(texts)\n",
    " \n",
    "# Passo 2: Padronizar o comprimento das sequências (deixar todas com o mesmo tamanho)\\n\",\n",
    "    padded_vetores = pad_sequences(vetores)  # Deixar todas as sequências com tamanho 10\\n\",\n",
    " \n",
    "# Verificar o resultado da tokenização e padding\\n\",\n",
    "    print(padded_vetores)\n",
    "    \n",
    "    print('Texto vetorizado com sucesso!')\n",
    "   \n",
    "except Exception as e:\n",
    "     print('Erro ao vetorizar texto: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construindo a rede neural..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "\n",
    "# Definir as constantes para o modelo\\n\",\n",
    "try:\n",
    "    print('Construindo a rede neural...')\n",
    "# tamanho do vocabulário\\n\",\n",
    "    VOCAB_SIZE = len(tokenizer.word_index) + 1  \n",
    "\n",
    "# Comprimento máximo da sequência\\n\",\n",
    "    MAX_SEQUENCE_LENGTH = padded_vetores.shape[1]\n",
    "   \n",
    "# Tamanho do vetor\\n\",\n",
    "    VETOR_LENGHT = int(np.sqrt(VOCAB_SIZE))\n",
    "    \n",
    "# Construir o modelo sequencial\\n\",\n",
    "    model = Sequential()\n",
    "  \n",
    "# Camada de entrada\n",
    "    model.add(Embedding, input_dim=VOCAB_SIZE, input_dim=VOCAB_SIZE,output_dim=VETOR_LENGHT,input_length=MAX_SEQUENCE_LENGTH)\n",
    "          \n",
    "# Camada LSTM para capturar a sequência do texto\n",
    "# rodar primeiro somente com 128\n",
    "# model.add(LSTM(128))\n",
    "    \n",
    "# rodar com mais camadas\n",
    "# return_sequences: parâmetro que indica se a camada deve retornar\n",
    "# a sequência inteira para a próxima camada\n",
    "# sequencia inteira é a sequência de saídas de cada unidade de tempo\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    \n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    \n",
    "# última camada LSTM não precisa retornar a sequência inteira\n",
    "    model.add(LSTM(32))          \n",
    "                                 \n",
    "# Camada final para prever a nota (classificação)\n",
    "# Definir a quantidade de neurônios de acordo \n",
    "# com a quantidade de classes/categorias\n",
    "    model.add(Dense(5))\n",
    "# construir o modelo\\n\",\n",
    "    model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n",
    "   \n",
    "# Usar um otimizador Adam com uma taxa de aprendizado menor\n",
    "# Adam: é um otimizador que ajusta a taxa de aprendizado\n",
    "    otimizador = Adam(learning_rate=0.0001) \n",
    "  \n",
    "# Compilar o modelo\n",
    "# loss precisa ser ajustado de acordo com o tipo de problema\n",
    "# nesse caso, é um problema de classificação\n",
    "# sparse_categorical_crossentropy: função de perda para classificação\n",
    "# de múltiplas classes/categorias\n",
    "# sparse: indica que os rótulos são inteiros\n",
    "# categorical: indica que os rótulos são one-hot encoded, ou seja,\n",
    "# cada rótulo é uma categoria\n",
    "# crossentropy: é a função de perda que mede a diferença entre a distribuição\n",
    "# de probabilidade prevista pelo modelo e a distribuição de probabilidade real\n",
    "# nesse caso observa-se q a perda durante as épocas\n",
    "# Se a perda estiver diminuindo, o modelo está aprendendo\n",
    "# Se a perda estiver aumentando, o modelo não está aprendendo e pode estar\n",
    "# ocorrendo overfitting\n",
    "    \n",
    "    model.compile(optimizer=otimizador, loss='sparse_categorical_crossentropy')    \n",
    "\n",
    "    # Verificar o resumo do modelo\n",
    "                   #model.summary()\n",
    "except Exception as e:\n",
    "         print('Erro ao construir a rede neural: ',e)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinar a rede neural\n",
    "try:\n",
    "    print('Treinando a rede neural...')\n",
    "   \n",
    "# Dividir os dados em treino e teste (80% treino, 20% teste)\n",
    "    from sklearn.model_selection import train_test_split # type: ignore\n",
    "    X_train, X_test, y_train, y_test = train_test_split\n",
    "    padded_vetores, # textos vetorizados\n",
    "    ratings, # notas\\n\",\n",
    "    test_size=0.2, # 20% dos dados para teste\n",
    "    random_state=42 # seed para reprodução dos resultados\n",
    "   \n",
    "# Ajuste temporário dos rótulos para o intervalo [0, 4] para o treinamento\n",
    "# Visto que a classificação começa em 1 e termina em 5\n",
    "# e a classificação começa em 0\\n\",\n",
    "    y_train_adjusted = y_train - 1\n",
    "    y_test_adjusted = y_test - 1\n",
    "   \n",
    "# Aumentar o número de épocas para dar mais tempo ao modelo para aprender\n",
    "# Aumentar o tamanho do batch para acelerar o treinamento, porém,\n",
    "# pode ser que o modelo não aprenda bem\n",
    "# Reduzir o tamanho do batch para dar mais tempo ao modelo para aprender\n",
    "# e para evitar overfitting e aprender melhor\n",
    "    model.fit(\n",
    "            X_train, # textos de treino \n",
    "                           y_train_adjusted, # notas de treino\n",
    "                           epochs=10, # número de épocas \n",
    "                            batch_size=32, # tamanho do batch de treino\n",
    "                            validation_data=(X_test, y_test_adjusted)\n",
    "                           )\n",
    "    \n",
    "except Exception as e:\n",
    "        print('Erro ao treinar a rede neural: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Realizando previsões...')\n",
    "# Exemplos de novos textos para testar o modelo\\n\",\n",
    "    novos_textos = [\n",
    "                       \"Muito bom, gostei bastante. Top demais! Compensa muito\\\",\\n\",\n",
    "                        \"Não recomendo, péssimo produto. Não funciona\\\",\\n\",\n",
    "                    ]\n",
    "   \n",
    "# Vetorizar e padronizar os novos textos\\n\",\n",
    "    novas_sequencias = tokenizer.texts_to_sequences(novos_textos)\n",
    "    novas_sequencias_padded = pad_sequences(novas_sequencias)\n",
    "   \n",
    "# Fazer previsões\\n\",\n",
    "    predicoes = model.predict(novas_sequencias_padded)\n",
    "   \n",
    "# Exibir as previsões\\n\",\n",
    "    print(\"Previsões:\", predicoes + 1)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Erro ao realizar previsões: ',e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
